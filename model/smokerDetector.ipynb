{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-10T08:19:57.459985Z",
     "iopub.status.busy": "2025-05-10T08:19:57.459224Z",
     "iopub.status.idle": "2025-05-10T08:19:57.465647Z",
     "shell.execute_reply": "2025-05-10T08:19:57.464895Z",
     "shell.execute_reply.started": "2025-05-10T08:19:57.459962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "from tensorflow.keras.applications import MobileNetV2,EfficientNetV2B0\n",
    "from tensorflow.keras import layers, models, Sequential, callbacks, optimizers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Ensure TensorFlow and CuDNN operations are deterministic\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Ensure using the correct GPU\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:19:57.467674Z",
     "iopub.status.busy": "2025-05-10T08:19:57.467093Z",
     "iopub.status.idle": "2025-05-10T08:19:57.500404Z",
     "shell.execute_reply": "2025-05-10T08:19:57.499461Z",
     "shell.execute_reply.started": "2025-05-10T08:19:57.467653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)  # Set seed for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:19:57.501314Z",
     "iopub.status.busy": "2025-05-10T08:19:57.501090Z",
     "iopub.status.idle": "2025-05-10T08:19:57.514475Z",
     "shell.execute_reply": "2025-05-10T08:19:57.513817Z",
     "shell.execute_reply.started": "2025-05-10T08:19:57.501299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "TARGET_HEIGHT = 244\n",
    "TARGET_WIDTH = 244\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 500\n",
    "IMAGE_DIMENSIONS = (244,244)\n",
    "\n",
    "#add the path to the dataset\n",
    "train_dir = 'smoking2/Training/Training'\n",
    "val_dir = 'smoking2/Validation/Validation'\n",
    "test_dir = 'smoking2/Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:19:57.515284Z",
     "iopub.status.busy": "2025-05-10T08:19:57.515048Z",
     "iopub.status.idle": "2025-05-10T08:19:57.527722Z",
     "shell.execute_reply": "2025-05-10T08:19:57.527111Z",
     "shell.execute_reply.started": "2025-05-10T08:19:57.515263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    \n",
    "    def preprocess_image(image, label):\n",
    "        # Convert image to float32\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        # Resize image\n",
    "        image = tf.image.resize(image, [TARGET_HEIGHT, TARGET_WIDTH])\n",
    "        # Apply random contrast with a chosen range\n",
    "        image = tf.image.random_contrast(image, lower = 0.50, upper = 1.50)\n",
    "        image = tf.image.random_saturation(image, lower = 0.50, upper = 1.50)\n",
    "        image = tf.image.random_hue(image, 0.2)\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "        # Normalize to keep pixel values between 0 and 1\n",
    "        image_min = tf.reduce_min(image)\n",
    "        image_max = tf.reduce_max(image)\n",
    "        \n",
    "        # Scale the image to [0, 1] if the max-min > 0 to avoid division by zero\n",
    "        image = tf.cond(\n",
    "            tf.greater(image_max - image_min, 0),\n",
    "            lambda: (image - image_min) / (image_max - image_min),\n",
    "            lambda: image  # If the image is constant, just return it as is\n",
    "        )\n",
    "\n",
    "        #################################\n",
    "        image = tf.keras.applications.efficientnet_v2.preprocess_input(image)\n",
    "\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "    # Preprocess images, cache, and prefetch\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:19:57.529659Z",
     "iopub.status.busy": "2025-05-10T08:19:57.529467Z",
     "iopub.status.idle": "2025-05-10T08:20:00.723039Z",
     "shell.execute_reply": "2025-05-10T08:20:00.722235Z",
     "shell.execute_reply.started": "2025-05-10T08:19:57.529645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = image_dataset_from_directory(train_dir, image_size=IMAGE_DIMENSIONS, batch_size=BATCH_SIZE, label_mode='binary', shuffle=True, seed=seed)\n",
    "validation_dataset = image_dataset_from_directory(val_dir, image_size=IMAGE_DIMENSIONS, batch_size=BATCH_SIZE, label_mode='binary', shuffle=False, seed=seed)\n",
    "test_dataset = image_dataset_from_directory(test_dir, image_size=IMAGE_DIMENSIONS, batch_size=BATCH_SIZE, shuffle=False, seed=seed)\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "\n",
    "# Preprocess the datasets\n",
    "train_dataset = preprocess(train_dataset)\n",
    "validation_dataset = preprocess(validation_dataset)\n",
    "test_dataset = preprocess(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:20:00.724077Z",
     "iopub.status.busy": "2025-05-10T08:20:00.723863Z",
     "iopub.status.idle": "2025-05-10T08:20:06.474772Z",
     "shell.execute_reply": "2025-05-10T08:20:06.474212Z",
     "shell.execute_reply.started": "2025-05-10T08:20:00.724055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_labels(dataset, alpha=0.60, scaling_strategy='log', base_multiplier=0.50):\n",
    "    \n",
    "    label_counts = Counter()\n",
    "    \n",
    "    # Count label occurrences batch-wise\n",
    "    for _, labels in dataset:  \n",
    "        label_indices = np.argmax(labels.numpy(), axis=1)  # Convert one-hot to label indices\n",
    "        label_counts.update(label_indices)\n",
    "    \n",
    "    # Create a list of labels and their counts\n",
    "    all_labels = np.concatenate([y.numpy() for _, y in train_dataset.take(-1)]).flatten()\n",
    "    all_counts = [count for label, count in label_counts.items()]\n",
    "    \n",
    "    # Compute base class weights (inverse proportional to class frequency)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                         classes=np.unique(all_labels), \n",
    "                                         y=all_labels)\n",
    "    \n",
    "    # Apply custom scaling strategies\n",
    "    if scaling_strategy == 'power':\n",
    "        # Raise weights to a power to control the effect of imbalance\n",
    "        class_weights = np.power(class_weights, alpha)\n",
    "    elif scaling_strategy == 'log':\n",
    "        # Apply logarithmic scaling to the class weights\n",
    "        class_weights = np.log1p(class_weights)\n",
    "    \n",
    "    # Multiply by the base multiplier\n",
    "    class_weights *= base_multiplier\n",
    "    \n",
    "    # Convert class weights to a dictionary format\n",
    "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    \n",
    "    return class_weights_dict\n",
    "\n",
    "# Count the labels in the dataset\n",
    "class_weights_dict = count_labels(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:20:06.475706Z",
     "iopub.status.busy": "2025-05-10T08:20:06.475499Z",
     "iopub.status.idle": "2025-05-10T08:20:06.480046Z",
     "shell.execute_reply": "2025-05-10T08:20:06.479352Z",
     "shell.execute_reply.started": "2025-05-10T08:20:06.475691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:20:06.481171Z",
     "iopub.status.busy": "2025-05-10T08:20:06.480909Z",
     "iopub.status.idle": "2025-05-10T08:20:06.492571Z",
     "shell.execute_reply": "2025-05-10T08:20:06.491958Z",
     "shell.execute_reply.started": "2025-05-10T08:20:06.481145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:20:06.494727Z",
     "iopub.status.busy": "2025-05-10T08:20:06.494497Z",
     "iopub.status.idle": "2025-05-10T08:20:06.506079Z",
     "shell.execute_reply": "2025-05-10T08:20:06.505543Z",
     "shell.execute_reply.started": "2025-05-10T08:20:06.494712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_efficientnet_model(num_classes=1):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomRotation(0.5),  # Reduced from 0.1\n",
    "        layers.RandomZoom(0.3),      # Reduced from 0.1\n",
    "        layers.RandomTranslation(0.3, 0.3),  # Reduced from 0.1\n",
    "        # layers.RandomContrast(0.2)\n",
    "    ], name=\"data_augmentation\")\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(TARGET_HEIGHT, TARGET_WIDTH, 3)),\n",
    "        data_augmentation,\n",
    "            \n",
    "        layers.Flatten(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.03)),  # Increased regularization\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),  # Increased dropout\n",
    "        \n",
    "        # Simplified architecture with fewer layers and neurons\n",
    "        \n",
    "        layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.03)),  # Increased regularization\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),  # Increased dropout\n",
    "\n",
    "        layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.03)),  # Increased regularization\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),  # Increased dropout\n",
    "\n",
    "        \n",
    "        layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "\n",
    "    # Compile model with reduced learning rate\n",
    "    initial_learning_rate = 1e-4  # Reduced from 5e-4\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=initial_learning_rate, \n",
    "            weight_decay=0.1  # Increased weight decay\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC()]  # Added AUC metric\n",
    "    )\n",
    "    \n",
    "    # Return only the model, not a tuple\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:20:29.599330Z",
     "iopub.status.busy": "2025-05-10T08:20:29.598352Z",
     "iopub.status.idle": "2025-05-10T08:30:57.359446Z",
     "shell.execute_reply": "2025-05-10T08:30:57.358825Z",
     "shell.execute_reply.started": "2025-05-10T08:20:29.599296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "########################################\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "lr_schedule = callbacks.LearningRateScheduler(lambda epoch: float(1e-3 * tf.math.exp(-0.04 * epoch)), verbose=1)\n",
    "reduce_lr_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=10, verbose=1)\n",
    "model_checkpoint = callbacks.ModelCheckpoint('model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "model = create_efficientnet_model(NUM_CLASSES)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, lr_schedule, reduce_lr_on_plateau, model_checkpoint],\n",
    "    # class_weight={0: 1, 1: 1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:31:07.357861Z",
     "iopub.status.busy": "2025-05-10T08:31:07.357347Z",
     "iopub.status.idle": "2025-05-10T08:31:07.369549Z",
     "shell.execute_reply": "2025-05-10T08:31:07.368800Z",
     "shell.execute_reply.started": "2025-05-10T08:31:07.357837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# from metrics_calculator import calculate_metrics\n",
    "\n",
    "def calculate_metrics(model, dataset, dataset_name=\"Dataset\", show_predictions=False, num_examples=5):\n",
    "    y_true = []\n",
    "    y_pred_probs = []\n",
    "    images_to_show = []\n",
    "    labels_to_show = []\n",
    "    probs_to_show = []\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataset):\n",
    "        batch_pred_probs = model.predict(images, verbose=0)\n",
    "\n",
    "        y_true.append(labels.numpy())\n",
    "        y_pred_probs.append(batch_pred_probs)\n",
    "\n",
    "        if show_predictions and len(images_to_show) < num_examples:\n",
    "            images_to_show.extend(images.numpy())\n",
    "            labels_to_show.extend(labels.numpy())\n",
    "            probs_to_show.extend(batch_pred_probs)\n",
    "\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred_probs = np.concatenate(y_pred_probs, axis=0)\n",
    "\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    if len(y_pred.shape) > 1:\n",
    "        if y_pred.shape[1] > 1:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "        else:\n",
    "            y_pred = y_pred.flatten()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if len(y_pred_probs.shape) > 1 and y_pred_probs.shape[1] > 1:\n",
    "        y_pred_prob = y_pred_probs[:, 1]\n",
    "    else:\n",
    "        y_pred_prob = y_pred_probs\n",
    "\n",
    "    print(f\"\\n===== {dataset_name} Metrics =====\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Smoking','Not Smoking'],\n",
    "                yticklabels=['Smoking','Not Smoking'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{dataset_name} Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if show_predictions:\n",
    "        class_names = ['Smoking','Not Smoking']\n",
    "        plt.figure(figsize=(15, 3 * num_examples))\n",
    "        for idx in range(min(num_examples, len(images_to_show))):\n",
    "            img = images_to_show[idx]\n",
    "            true_label = labels_to_show[idx]\n",
    "            prob = 1-probs_to_show[idx]\n",
    "            predicted_class = (prob < 0.5).astype(int)\n",
    "\n",
    "            plt.subplot(num_examples, 1, idx + 1)\n",
    "            plt.imshow(img.squeeze(), cmap='gray' if img.shape[-1] == 1 else None)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Predicted: {class_names[int(predicted_class)]} \"\n",
    "                      f\"({float(prob):.2f}) | True: {class_names[int(true_label)]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:31:10.295479Z",
     "iopub.status.busy": "2025-05-10T08:31:10.295234Z",
     "iopub.status.idle": "2025-05-10T08:31:10.650399Z",
     "shell.execute_reply": "2025-05-10T08:31:10.649719Z",
     "shell.execute_reply.started": "2025-05-10T08:31:10.295463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:31:13.715298Z",
     "iopub.status.busy": "2025-05-10T08:31:13.715007Z",
     "iopub.status.idle": "2025-05-10T08:31:19.570675Z",
     "shell.execute_reply": "2025-05-10T08:31:19.569873Z",
     "shell.execute_reply.started": "2025-05-10T08:31:13.715277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_metrics = calculate_metrics(model, test_dataset, \"Test\", show_predictions=True, num_examples=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T08:31:22.468069Z",
     "iopub.status.busy": "2025-05-10T08:31:22.467792Z",
     "iopub.status.idle": "2025-05-10T08:31:22.683258Z",
     "shell.execute_reply": "2025-05-10T08:31:22.682417Z",
     "shell.execute_reply.started": "2025-05-10T08:31:22.468047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read and preprocess the image\n",
    "img = tf.io.read_file('/kaggle/input/smoking2/model/Training/Training/notSmoking/notsmoking_0008.jpg')\n",
    "img = tf.image.decode_jpeg(img, channels=3)\n",
    "img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "img = tf.image.resize(img, [TARGET_HEIGHT, TARGET_WIDTH])\n",
    "\n",
    "# Add batch dimension\n",
    "img = tf.expand_dims(img, axis=0)  # Shape becomes (1, 80, 80, 3)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(img)\n",
    "print(\"Raw prediction:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7325247,
     "sourceId": 11756150,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
